{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping NFL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be scraping data from [Pro Football Reference](https://www.pro-football-reference.com/). They're the absolute best at NFL statistics, in my opinion. We will want to get:\n",
    "\n",
    "- All MVP voting data (this will tell us the share of the vote each MVP candidate received, as well as their statistics)\n",
    "- All player data\n",
    "- All team data\n",
    "\n",
    "We'll only collect data from 1990 to 2021. 31 years of MVP candidates is probably good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1990, 2022))\n",
    "\n",
    "mvp_voting_url = 'https://www.pro-football-reference.com/awards/awards_{}.htm'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not familiar with web scraping, what we'll be doing in a nutshell is downloading all of the HTML files that contain the data we need, then using `BeautifulSoup` to pull out the tables that we want.\n",
    "\n",
    "First, we'll create a folder called 'mvp' where we'll store our HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('mvp')\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will download the MVP voting information for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mvp_html():\n",
    "    for year in years:\n",
    "        url = mvp_voting_url.format(year)\n",
    "        r = requests.get(url)\n",
    "        \n",
    "        with open('mvp/mvp_voting_{}.html'.format(year), 'w') as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "# download_mvp_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to parse the HTML. The MVP voting table has a unique ID called \"voting_apmvp\", so we'll use that. The tables also have an over-header that we don't need, so we'll remove that. It's important that we add a 'Year' column so we can distinguish the year that the MVP data is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    with open('mvp/mvp_voting_{}.html'.format(year)) as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "    soup.find('tr', {'class': 'over_header'}).decompose()\n",
    "    \n",
    "    table = soup.find('table', {'id': 'voting_apmvp'})\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    df['Year'] = year\n",
    "    \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine the data frames we generated above and save them as a CSV so we don't have to keep calling the website for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat(dfs).to_csv('mvp_voting.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player stats are going to be a bit more complicated, mainly because A LOT of people have played football in the NFL. So, we'll need to download a list of all players:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_names_url = 'https://www.pro-football-reference.com/players/{}/'\n",
    "\n",
    "try:\n",
    "    os.mkdir('players')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "alphabet = list(map(chr, range(65, 91)))\n",
    "\n",
    "def download_player_list_html():\n",
    "    for letter in alphabet:\n",
    "        url = player_names_url.format(letter)\n",
    "        r = requests.get(url)\n",
    "\n",
    "        with open('players/{}.html'.format(letter), 'w') as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "# download_player_list_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to save each link to players who played from 1990 onward. Each player in the list has a link to their specific stats page, as well as the years they've played, so with a little RegEx, this is pretty straight forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 11805 players in the NFL since 1990\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "for letter in alphabet:\n",
    "    with open('players/{}.html'.format(letter)) as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "    \n",
    "    # find all <p> in <div> with id 'div_players'\n",
    "    for player in soup.find('div', {'id': 'div_players'}).find_all('p'):\n",
    "        re_result = re.search(r'(\\d{4})-\\d{4}', player.text)\n",
    "        start_year = int(re_result.group(1))\n",
    "\n",
    "        if (start_year >= years[0]):\n",
    "            link = player.find('a')['href'].replace('.htm', '/gamelog/')\n",
    "            links.append(link)\n",
    "\n",
    "print(\"There have been\", len(links), \"players in the NFL since\", years[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of players! Now, we can download the gamelog for each player (note, this takes a VERY long time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_player_html():\n",
    "    try:\n",
    "        os.mkdir('gamelogs')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    index = 0\n",
    "\n",
    "    for link in links:\n",
    "        r = requests.get('https://www.pro-football-reference.com' + link) \n",
    "\n",
    "        with open('gamelogs/{}.html'.format(index), 'w') as f:\n",
    "            f.write(r.text)\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "# download_player_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to parse the stats table from each HTML page, as well as getting the players' birthday and name (this is important in the case of two players having the same name, we'd hope they have different birthdays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_of_all_players():\n",
    "    player_dfs = []\n",
    "\n",
    "    for index in range(0, 11805):\n",
    "        with open('gamelogs/{}.html'.format(index)) as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "            if soup.find('tr', {'class': 'over_header'}):\n",
    "                soup.find('tr', {'class': 'over_header'}).decompose()\n",
    "\n",
    "            if soup.find('tr', {'class': 'thead'}):\n",
    "                for thead in soup.find_all('tr', {'class': 'thead'}):\n",
    "                    thead.decompose()\n",
    "\n",
    "            if soup.find('table', {'class': 'stats_table'}):\n",
    "                table = soup.find('table', {'class': 'stats_table'})\n",
    "                df = pd.read_html(str(table))[0]\n",
    "\n",
    "                # get value of h1 with itemprop 'name'\n",
    "                if soup.find('h1', {'itemprop': 'name'}):\n",
    "                    df['Name'] = soup.find('h1', {'itemprop': 'name'}).text.strip()\n",
    "\n",
    "                if soup.find('span', {'data-birth': True}):\n",
    "                    df['Birthday'] = soup.find('span', {'data-birth': True})['data-birth']\n",
    "\n",
    "                player_dfs.append(df)\n",
    "\n",
    "    df = pd.concat(player_dfs)\n",
    "    df.to_csv('player_stats.csv.gz', compression='gzip')\n",
    "\n",
    "#create_df_of_all_players()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the same thing with team stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_url = 'https://www.pro-football-reference.com/boxscores/standings.cgi?week=18&year={}&wk_league=NFL'\n",
    "\n",
    "def download_team_stats_html():\n",
    "    try:\n",
    "        os.mkdir('teams')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    for year in years:\n",
    "        url = team_stats_url.format(year)\n",
    "        r = requests.get(url)\n",
    "\n",
    "        with open('teams/{}.html'.format(year), 'w') as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "# download_team_stats_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_of_all_teams():\n",
    "    team_dfs = []\n",
    "\n",
    "    for year in years:\n",
    "        with open('teams/{}.html'.format(year)) as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "\n",
    "            if soup.find('tr', {'class': 'over_header'}):\n",
    "                soup.find('tr', {'class': 'over_header'}).decompose()\n",
    "\n",
    "            if soup.find('tr', {'class': 'thead'}):\n",
    "                for thead in soup.find_all('tr', {'class': 'thead'}):\n",
    "                    thead.decompose()\n",
    "\n",
    "            if soup.find('table', {'id': 'AFC'}):\n",
    "                table_afc = soup.find('table', {'id': 'AFC'})\n",
    "                df = pd.read_html(str(table_afc))[0]\n",
    "                df['Year'] = year\n",
    "\n",
    "                team_dfs.append(df)\n",
    "            \n",
    "            if soup.find('table', {'id': 'NFC'}):\n",
    "                table_nfc = soup.find('table', {'id': 'NFC'})\n",
    "                df = pd.read_html(str(table_nfc))[0]\n",
    "                df['Year'] = year\n",
    "\n",
    "                team_dfs.append(df)\n",
    "\n",
    "    df = pd.concat(team_dfs)\n",
    "    df.to_csv('team_stats.csv')\n",
    "\n",
    "# create_df_of_all_teams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're done! We've collected all the data we need."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae77cee08a405d5a35a2a715fe9ca5db87c693d27e7db0b272246dc1c5f64789"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
